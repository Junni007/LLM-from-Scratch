# Models Directory

This directory contains implementations of various model components for the LLM from Scratch project.

## Components

### Attention Mechanisms
- [attention.py](attention.py): Implementation of scaled dot-product attention, single attention head, and multi-head attention

### Feed-Forward Networks
- [mlp.py](mlp.py): Implementation of feed-forward networks (MLP layers)

### Normalization
- [normalization.py](normalization.py): Implementation of LayerNorm

### Transformer Blocks
- [transformer.py](transformer.py): Implementation of Transformer blocks

### Tokenizers
- [tokenizers/](../tokenizers/): Various tokenization implementations

## Implementation Progress

- [x] Scaled Dot-Product Attention
- [x] Single Attention Head
- [x] Multi-Head Attention
- [x] Feed-Forward Networks
- [x] Layer Normalization
- [x] Residual Connections
- [x] Full Transformer Block