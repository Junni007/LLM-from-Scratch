"""
RLHF (Reinforcement Learning from Human Feedback) package.
Contains implementations of PPO and GRPO algorithms for LLM alignment.
"""