{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny LLM Example\n",
    "\n",
    "This notebook demonstrates a complete example of training a tiny LLM using the components we've implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from src.models.transformer import TransformerBlock\n",
    "from src.tokenizers.byte_tokenizer import ByteTokenizer\n",
    "from src.train.data import create_data_loader\n",
    "from src.train.trainer import Trainer\n",
    "from src.train.evaluation import evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    A tiny LLM implementation using our Transformer blocks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=256, d_model=64, num_heads=4, num_layers=2, d_ff=128):\n",
    "        super(TinyLLM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply output projection\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = ByteTokenizer()\n",
    "\n",
    "# Sample training data\n",
    "training_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning models have revolutionized many fields.\",\n",
    "    \"Transformers are a powerful architecture for sequence modeling.\",\n",
    "    \"Attention mechanisms allow models to focus on relevant parts of input.\",\n",
    "    \"Large language models can generate human-like text.\",\n",
    "    \"Tokenization is the process of converting text into tokens.\",\n",
    "    \"Neural networks learn patterns from data.\",\n",
    "    \"PyTorch is a popular deep learning framework.\"\n",
    "]\n",
    "\n",
    "# Encode texts\n",
    "tokenized_texts = tokenizer.encode_batch(training_texts, add_bos=True, add_eos=True)\n",
    "\n",
    "# Create data loader\n",
    "seq_length = 16\n",
    "batch_size = 4\n",
    "data_loader = create_data_loader(tokenized_texts, seq_length, batch_size)\n",
    "\n",
    "print(f\"Number of training samples: {len(data_loader.dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = TinyLLM(vocab_size=tokenizer.vocab_size, d_model=64, num_heads=4, num_layers=2)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few epochs\n",
    "num_epochs = 3\n",
    "trainer.train(data_loader, num_epochs, log_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "metrics = evaluate_model(model, data_loader)\n",
    "print(f\"Evaluation Metrics:\")\n",
    "print(f\"  Loss: {metrics['loss']:.4f}\")\n",
    "print(f\"  Perplexity: {metrics['perplexity']:.4f}\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text (Simple Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, add_bos=True)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get logits from model\n",
    "            logits = model(input_tensor)\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Sample next token\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            \n",
    "            # Append to input\n",
    "            input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Stop if we generate an EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Decode generated text\n",
    "    generated_ids = input_tensor[0].tolist()\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Generate some text\n",
    "prompt = \"Machine learning\"\n",
    "generated_text = generate_text(model, tokenizer, prompt, max_length=30)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}